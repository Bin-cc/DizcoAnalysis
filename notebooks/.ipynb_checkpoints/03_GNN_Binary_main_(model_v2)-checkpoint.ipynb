{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7362e4-7f5d-44c7-8d35-12a638cbee17",
   "metadata": {},
   "source": [
    "##### 构建蛋白的图模型，以GNN为核心，执行二分类任务（蛋白的氨基酸是否有被标记，或蛋白是否被标记）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ae120-d58f-431d-81e7-359247e45eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:/Proteomics/PhD_script/1. Dizco/')\n",
    "sys.path.append('D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/Ligand Discovery/fragment-embedding/')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from fragmentembedding import FragmentEmbedder\n",
    "from Protein2Graph import prot2graph\n",
    "from StructureInformationIntegration import data_integration\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,precision_score,recall_score,f1_score,accuracy_score,matthews_corrcoef,average_precision_score,brier_score_loss,auc,precision_recall_curve\n",
    "from models import dizco_GNN\n",
    "from torch.optim import lr_scheduler\n",
    "from EarlyStopping import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from rdkit import RDLogger\n",
    "import pickle\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301e23e-d7ae-492f-8909-996f024bd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对蛋白氨基酸序列的每一个残基说明是否有被探针标记\n",
    "def ResLabel(table,thre=0.85):\n",
    "    label_table = table[table['lg_probs']>thre]\n",
    "    label_table = label_table[~label_table['label_site'].isna()]\n",
    "    unlabel_table = table[table['lg_probs']<=thre]\n",
    "    \n",
    "    labeledPEP = ParseRes(label_table,state='labeled')\n",
    "    unlabeledPEP = ParseRes(unlabel_table,state='unlabeled')\n",
    "    res_table = pd.concat([labeledPEP,unlabeledPEP],axis=0)\n",
    "    res_table = res_table.sort_values(by='label',ascending=False).reset_index(drop=True)\n",
    "    res_table = res_table.drop_duplicates(subset=['Accession','res_site']).reset_index(drop=True)\n",
    "    \n",
    "    prot_lt = list(res_table['Accession'].unique())\n",
    "    prot_label = {}\n",
    "    for prot in prot_lt:\n",
    "        seq = uniprot_infor[uniprot_infor['Entry']==prot].iloc[0,-1]\n",
    "        prot_dict = {f'{p}{i+1}':np.nan for i,p in enumerate(seq)}\n",
    "        prot_label.setdefault(prot,prot_dict)\n",
    "    \n",
    "    for prot,table in res_table.groupby(by='Accession'):\n",
    "        for site,label in zip(table['res_site'].to_list(),table['label'].to_list()):\n",
    "            prot_label[prot][site] = label\n",
    "\n",
    "    return prot_label\n",
    "\n",
    "#根据dizco的数据确认残基是否有被标记，若是则标为1，否则标为0\n",
    "#该标记可进一步延伸为蛋白层面上的标记，即若某一蛋白中任一残基被标为1，则该蛋白也被标为1，否则标为0\n",
    "def ParseRes(data,state='labeled'):\n",
    "    pep_label_result = pd.DataFrame()\n",
    "    for (prot,pep),table in data.groupby(by=['Master Protein Accessions','Upper_Seq']):\n",
    "        try: seq = uniprot_infor[uniprot_infor['Entry']==prot].iloc[0,-1]\n",
    "        except: continue\n",
    "\n",
    "        start = seq.find(pep)+1\n",
    "        label_site = list(table['label_site'].unique())\n",
    "        pep_label = []\n",
    "        for i,p in enumerate(pep):\n",
    "            if state == 'labeled':\n",
    "                if f'{p}{i+1}' in label_site:\n",
    "                    pep_label.append(tuple((prot,f'{p}{start+i}',1)))\n",
    "                else: pep_label.append(tuple((prot,f'{p}{start+i}',0)))\n",
    "            elif state == 'unlabeled' : pep_label.append(tuple((prot,f'{p}{start+i}',0)))\n",
    "        pep_label = pd.DataFrame(pep_label,columns=['Accession','res_site','label'])\n",
    "        pep_label_result = pd.concat([pep_label_result,pep_label],axis=0)\n",
    "    pep_label_result = pep_label_result.sort_values(by='label',ascending=False).reset_index(drop=True)\n",
    "   \n",
    "    return pep_label_result.drop_duplicates(subset=['Accession','res_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f00cd-1a9f-4d46-ac60-60043d9dcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将蛋白的accession name转为Alphafold的pdb文件名\n",
    "def prot2pdb(prot_list,pdb_path):\n",
    "    prot_pdb_list,prot_list_ = [],[]\n",
    "    for name in listdir(pdb_path):\n",
    "        if any(prot in name for prot in prot_list):\n",
    "            if name.split('-')[1] not in prot_list_:\n",
    "                prot_pdb_list.append(name)\n",
    "                prot_list_.append(name.split('-')[1])\n",
    "    return prot_pdb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a4c46-0692-4f1a-bc45-698e720187b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集按8:1:1划分\n",
    "def DataSetSplit(dataSet):\n",
    "    data_index = np.array(range(len(dataSet)))\n",
    "    \n",
    "    tv_index, test_index = train_test_split(data_index,test_size=0.1,random_state=42,shuffle=True)\n",
    "    train_index, val_index = train_test_split(tv_index,test_size=1/9,random_state=42,shuffle=True)\n",
    "    extract = lambda x:[dataSet[i] for i in x]\n",
    "    train_set,val_set,test_set = extract(train_index),extract(val_index),extract(test_index)\n",
    "    train_set = DataCombine(train_set)\n",
    "    val_set = DataCombine(val_set)\n",
    "    test_set = DataCombine(test_set,sample_type='test_set')\n",
    "    \n",
    "    return train_set,val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095ce10-9a85-4b5a-9591-9324deafc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并蛋白的图数据，探针的embedding数据以及label数据\n",
    "def DataCombine(data,sample_type=None):\n",
    "    graph_set,probe_emd_set,label_set = [],[],[]\n",
    "    for i,samples in enumerate(data):\n",
    "        prot_graph,probeEmbedding,labels = samples\n",
    "        probeEmbedding = np.asarray(probeEmbedding).astype(np.float32)\n",
    "        labels = np.asarray(labels).astype(np.float32)\n",
    "        \n",
    "        graph_set.append(prot_graph)\n",
    "        probe_emd_set.append(torch.tensor(probeEmbedding))\n",
    "        label_set.append(torch.tensor(labels))\n",
    "    \n",
    "    if sample_type is None:\n",
    "        graph_set = [tuple((i,graph)) for i,graph in enumerate(graph_set)]\n",
    "        graph_set = DataLoader(graph_set, batch_size=8, drop_last=True, shuffle=True)\n",
    "\n",
    "    return (graph_set,probe_emd_set,label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca16c6-cc9a-4dd6-8867-ab15bbf50b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在这里由于图模型和探针的embedding模型是分开加载的，因此需要保留batch_idx以调取对应批次的数据\n",
    "#分开加载的原因是因为在进行氨基酸层面的预测时，每个蛋白的probe embedding和label维度不一致，无法stack到一起\n",
    "#若进行蛋白层面的预测则无需如此操作\n",
    "def model_train(model,train_set,val_set,save_best_model=True,model_path=None,save_path=None,earlyStop=True,epochs=100,lr=0.0001,thre=0.5):\n",
    "    if save_path is not None: early_stopping = EarlyStopping(save_path)\n",
    "    else: earlyStop = False\n",
    "    metrics_name = ['train_loss','val_loss','acc','precision','recall','f1','ap','bsl','mcc','auc_score','prc_score']\n",
    "    #定义损失函数计算方法，定义优化器\n",
    "    #criterion = nn.binary_cross_entropy_with_logits()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-3)\n",
    "    scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=lr/10, max_lr=lr*10,\n",
    "                                      cycle_momentum=False,step_size_up=len(train_set[0]))\n",
    "    \n",
    "    prot_graphs,probe,labels = train_set\n",
    "    weights = []\n",
    "    for label in labels:\n",
    "        w = []\n",
    "        for l in label.tolist():\n",
    "            if l == 0: w.append(1)\n",
    "            elif l == 1: w.append(10)\n",
    "        weights.append(torch.tensor(w))\n",
    "    weight_increase = 1\n",
    "    \n",
    "    metrics_result = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx,graph_values in prot_graphs:\n",
    "            probe_values = torch.cat([probe[i] for i in batch_idx],axis=0)\n",
    "            label_values = torch.cat([labels[i] for i in batch_idx],axis=0)\n",
    "            weight_values = torch.cat([weights[i] for i in batch_idx],axis=0)\n",
    "            graph_values, probe_values, label_values, weight_values = graph_values.to(device), probe_values.to(device), label_values.to(device), weight_values.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(graph_values, probe_values)\n",
    "            loss = F.binary_cross_entropy_with_logits(output.squeeze(), label_values, weight=weight_values)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            pred_labels = (F.sigmoid(output) > thre).int()\n",
    "            mask = (pred_labels != label_values.unsqueeze(1))\n",
    "            node_nums = []\n",
    "            for idx,i in enumerate(batch_idx):\n",
    "                num = len(labels[i])\n",
    "                if idx == 0: node_nums.append(tuple((0,num)))\n",
    "                else: node_nums.append(tuple((node_nums[idx-1][1],node_nums[idx-1][1]+num)))\n",
    "             \n",
    "            for i,(start,end) in zip(batch_idx,node_nums):\n",
    "                batch_mask = mask.tolist()[start:end]\n",
    "                batch_mask = [m[0] for m in batch_mask]\n",
    "                weights[i][batch_mask] += weight_increase\n",
    "                \n",
    "        train_loss = total_loss / len(labels)\n",
    "        eval_matrics = model_eval(model,val_set)\n",
    "        metrics_result.append(tuple((train_loss,))+eval_matrics)\n",
    "        \n",
    "        if earlyStop:\n",
    "            early_stopping(eval_matrics[0], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                return pd.DataFrame(metrics_result,columns=metrics_name)\n",
    "            break\n",
    "        \n",
    "        if epoch == 0:\n",
    "            best_mcc = eval_matrics[7]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_metrics = tuple((epoch,train_loss))+eval_matrics\n",
    "        else:\n",
    "            if eval_matrics[7] > best_mcc:\n",
    "                best_mcc = eval_matrics[7]\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_metrics = tuple((epoch,train_loss))+eval_matrics\n",
    "        \n",
    "    if save_best_model:\n",
    "        torch.save(best_model.state_dict(),model_path+'best_model.pth')\n",
    "        print('The metrics of best model are:')\n",
    "        print(f'epoch: {best_metrics[0]}, train_loss: {best_metrics[1]}, val_loss: {best_metrics[2]}')\n",
    "        print(f'acc: {best_metrics[3]}, precision: {best_metrics[4]}, recall: {best_metrics[5]}')\n",
    "        print(f'f1: {best_metrics[6]}, ap: {best_metrics[7]}, bsl: {best_metrics[8]}')\n",
    "        print(f'mcc: {best_metrics[9]}, AUC: {best_metrics[10]}, PRC: {best_metrics[11]}')\n",
    "        \n",
    "    return pd.DataFrame(metrics_result,columns=metrics_name)\n",
    "        \n",
    "def model_eval(model,val_set,thre=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true,y_pred,y_prob = [],[],[]\n",
    "    prot_graphs,probe,labels = val_set\n",
    "    \n",
    "    weights = []\n",
    "    for label in labels:\n",
    "        w = []\n",
    "        for l in label.tolist():\n",
    "            if l == 0: w.append(1)\n",
    "            elif l == 1: w.append(10)\n",
    "        weights.append(torch.tensor(w))\n",
    "    weight_increase = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,graph_values in prot_graphs:\n",
    "            probe_values = torch.cat([probe[i] for i in batch_idx],axis=0)\n",
    "            label_values = torch.cat([labels[i] for i in batch_idx],axis=0)\n",
    "            weight_values = torch.cat([weights[i] for i in batch_idx],axis=0)\n",
    "            graph_values, probe_values, label_values, weight_values = graph_values.to(device), probe_values.to(device), label_values.to(device), weight_values.to(device)\n",
    "            \n",
    "            output = model(graph_values, probe_values)\n",
    "            prob = output.squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(output.squeeze(), label_values, weight=weight_values)\n",
    "            total_loss += loss.item()\n",
    "            prob = F.sigmoid(prob)\n",
    "            pred = (prob >= thre).int()\n",
    "            y_true.extend(label_values.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "            y_prob.extend(prob.tolist())\n",
    "            \n",
    "            mask = (pred != label_values.unsqueeze(1))\n",
    "            node_nums = []\n",
    "            for idx,i in enumerate(batch_idx):\n",
    "                num = len(labels[i])\n",
    "                if idx == 0: node_nums.append(tuple((0,num)))\n",
    "                else: node_nums.append(tuple((node_nums[idx-1][1],node_nums[idx-1][1]+num)))\n",
    "             \n",
    "            for i,(start,end) in zip(batch_idx,node_nums):\n",
    "                batch_mask = mask.tolist()[start:end]\n",
    "                batch_mask = [m[0] for m in batch_mask]\n",
    "                weights[i][batch_mask] += weight_increase\n",
    "                \n",
    "        val_loss = total_loss/len(labels)\n",
    "        eval_matrics = metrics_calculation(y_true,y_pred,y_prob)\n",
    "        \n",
    "        return tuple((val_loss,))+eval_matrics\n",
    "\n",
    "def metrics_calculation(y_true,y_pred,y_prob):\n",
    "    y_true, y_pred, y_prob = np.float64(y_true), np.float64(y_pred), np.float64(y_prob)\n",
    "\n",
    "    auc_score = roc_auc_score(y_true,y_prob)\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    precision = precision_score(y_true,y_pred,average=None)[1]\n",
    "    recall = recall_score(y_true,y_pred)\n",
    "    f1 = f1_score(y_true,y_pred)\n",
    "    mcc = matthews_corrcoef(y_true,y_pred)\n",
    "    ap = average_precision_score(y_true,y_prob,average=None)\n",
    "    bsl = brier_score_loss(y_true,y_prob)\n",
    "    tpr,fpr,_ = precision_recall_curve(y_true,y_prob)\n",
    "    prc_score = auc(fpr,tpr)\n",
    "    \n",
    "    return tuple((acc,precision,recall,f1,ap,bsl,mcc,auc_score,prc_score))\n",
    "\n",
    "def metrics_plots(metrics,thre_col='mcc'):\n",
    "    best_epoch = metrics.sort_values(by=thre_col,ascending=False).index[0]\n",
    "    \n",
    "    #评估指标在每个epoch中的表现\n",
    "    plt.figure(figsize=(6,4),dpi=100)\n",
    "    for i in range(metrics.shape[1]):\n",
    "        if i < 2: continue\n",
    "        plt.plot(metrics.index,metrics.iloc[:,i],label=metrics.columns[i])\n",
    "    plt.axvline(x=best_epoch,color='black',linestyle='--')\n",
    "    plt.legend(bbox_to_anchor=(1,1))\n",
    "    plt.xlabel('Epoch',fontsize=12)\n",
    "    plt.ylabel('Score',fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    #每个epoch中损失情况\n",
    "    plt.figure(figsize=(6,4),dpi=100)\n",
    "    for i in range(metrics.shape[1]):\n",
    "        if i >= 2: continue\n",
    "        plt.plot(metrics.index,metrics.iloc[:,i],label=metrics.columns[i])\n",
    "    plt.axvline(x=best_epoch,color='black',linestyle='--')\n",
    "    plt.legend(bbox_to_anchor=(1,1))\n",
    "    plt.xlabel('Epoch',fontsize=12)\n",
    "    plt.ylabel('Loss',fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "def model_test(model,test_set):\n",
    "    model.eval()\n",
    "    prot_graphs,probe,labels = test_set\n",
    "    output = []\n",
    "    for graph_values, probe_values, label_values in zip(prot_graphs,probe,labels):\n",
    "        graph_values, probe_values = graph_values.to(device), probe_values.to(device)\n",
    "        test_output = model(graph_values,probe_values)\n",
    "        output.append(test_output.tolist())\n",
    "    return output,[i.tolist() for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e397131-1bdb-47a7-be9f-519fdb8c0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 把八种探针的PSM数据整合\n",
    "path = 'D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/'\n",
    "file_path = f'{path}Test files/'\n",
    "merge_PSM = pd.read_csv(f'{file_path}merge_isoTOP_PSM_data.csv')\n",
    "uniprot_infor = pd.read_csv(f'{file_path}uniprotkb_human_AND_reviewed_true_AND_m_2024_09_12.tsv',sep='\\t')\n",
    "uniprot_infor['Gene Names'] = uniprot_infor['Gene Names'].str.split(' ',expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e12c63-0fdc-42a3-a6e7-f29d11d04213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 为八种探针鉴定到的所有蛋白创建图\n",
    "pdb_path = f'{path}AlphaFold_pdbFiles/'\n",
    "processed_path = f'{path}/prot_raw_graph_coord/'\n",
    "prot_list = merge_PSM['Master Protein Accessions'].unique()\n",
    "prot_list = [prot for prot in prot_list if not uniprot_infor[uniprot_infor['Entry']==prot].empty]\n",
    "prot_pdb_list = prot2pdb(prot_list,pdb_path)\n",
    "\n",
    "pg = prot2graph(processed_path)\n",
    "with Pool(12) as p:\n",
    "    p.map(pg.CreateCoordGraph,tqdm(prot_pdb_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac422bf2-96da-4fea-baa6-943678ca7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 为八种探针鉴定到的所有蛋白总结结构信息\n",
    "with Pool(8) as p:\n",
    "    structure_data = p.map(data_integration,prot_pdb_list)\n",
    "structure_data_dic = {}\n",
    "for data in structure_data: structure_data_dic.update(data)\n",
    "\n",
    "pickle.dump(structure_data_dic,open(f'{path}pkl_files/structure_infor_summary.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c50308-8a5f-42ca-b3ba-06054a95de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 为每个探针生成一个embedding\n",
    "probe_smile = {\n",
    "    \n",
    "    'AJ5': 'C#CCCC1(N=N1)CCC(NC)=O',\n",
    "    'AJ8': 'C#CCCC1(N=N1)CCC(NC[C@H]2[C@@H](C)CCCN2CC3=CC=C(OC)C=C3)=O',\n",
    "    'AJ12': 'C#CCCC1(N=N1)CCNC(/C(C2=CC=CC=C2)=C/C3=CC=CC=C3)=O',\n",
    "    'AJ14': 'C#CCCC1(N=N1)CCC(NC(C2(C[C@H](C3)C4)C[C@H]4C[C@H]3C2)C)=O',\n",
    "    'AJ22': 'C#CCCC1(N=N1)CCNC(/C(CC)=C/C2=CC=CC([N+]([O-])=O)=C2)=O',\n",
    "    'AJ32': 'C#CCCC1(N=N1)CCNC(C2(CC2)C3=CC(OC(F)(F)O4)=C4C=C3)=O',\n",
    "    'AJ39': 'C#CCCC1(N=N1)CCNC(CCC2=NC(C3=CC=CC=C3)=C(C4=CC=CC=C4)O2)=O',\n",
    "    'CP78': 'C#CCCC1(N=N1)CCC(N2C(CC3=CC=CC=C3)CCCC2)=O'\n",
    "    \n",
    "    }\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "fe = FragmentEmbedder()\n",
    "probe_emd = {probe:fe.transform([smile]) for probe,smile in probe_smile.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f59b9-468f-4008-b597-966d8118a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 根据每个探针找到的肽段以及binding sites的差异，重新定义图，并整合数据\n",
    "#有的蛋白未收录在AlphaFold中，或许是由于uniprot更新了ID\n",
    "str_infor_dic = pickle.load(open(f'{path}pkl_files/structure_infor_summary.pkl','rb'))\n",
    "\n",
    "dataSet = []\n",
    "for probe,table in tqdm(merge_PSM.groupby(by='probe')):\n",
    "    if probe == 'AJ5': continue\n",
    "    prot_label = ResLabel(table,thre=0.85)\n",
    "    for prot,labels in prot_label.items():\n",
    "        try: prot_graph = torch.load(f'{processed_path}{prot}.pt')\n",
    "        except:\n",
    "            print(f'The graph of {prot} could not be found, please check whether there is a PDB file of it')\n",
    "            continue\n",
    "        node_update = np.array([tuple((int(res[1:])-1,label)) for res,label in labels.items() if not np.isnan(label)])\n",
    "        \n",
    "        try: new_edge,_ = subgraph(subset=torch.tensor(node_update[:,0]),edge_index=prot_graph.edge_index,relabel_nodes=True)\n",
    "        except:\n",
    "            print(f'{prot}')\n",
    "            print(f'No. res in PDB: {prot_graph.x.shape[0]}')\n",
    "            print(f'No. res in Uniprot: {len(labels)}')\n",
    "            continue\n",
    "            \n",
    "        new_X = prot_graph.x[torch.tensor(node_update[:,0])]\n",
    "        # if sum(node_update[:,1]!=0)>0: new_Y = 1\n",
    "        # else: new_Y = 0\n",
    "        new_Y = node_update[:,1]\n",
    "        new_graph = Data(x=new_X,edge_index=new_edge)\n",
    "        \n",
    "        probeEmbedding = probe_emd[probe]\n",
    "        probeEmbedding = probeEmbedding.repeat(len(node_update),axis=0)\n",
    "        \n",
    "        dataSet.append(tuple((new_graph,probeEmbedding,new_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a3203-ae6b-4768-a86f-7d7c9cd1ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 划分训练集、验证集和测试集\n",
    "train_set,val_set,test_set = DataSetSplit(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c454d-583b-4393-81ce-cd53c2706c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 模型训练与评估\n",
    "graph_dim,probe_dim = test_set[0][0].x.shape[1],test_set[1][0].shape[1]\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = 'E:/Proteomics/PhD_script/1. Dizco/'\n",
    "\n",
    "dizco_gnn = dizco_GNN(graph_dim,probe_dim).to(device)\n",
    "metrics_result = model_train(dizco_gnn,train_set,val_set,model_path=model_path,lr=0.0001)\n",
    "metrics_plots(metrics_result,thre_col='mcc')\n",
    "\n",
    "metrics_result.to_csv('D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/Model_Metrics_Result/Adam_lr(1e-5)_batch(16)(GNN).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127bf70-4098-4726-aec8-f361feed50c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733822a1-2baa-47e5-847c-34dfb92d19a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
