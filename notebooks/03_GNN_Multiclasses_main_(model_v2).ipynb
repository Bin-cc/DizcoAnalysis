{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e62cf48-ef08-48f5-a500-a8964c368fff",
   "metadata": {},
   "source": [
    "##### 构建蛋白的图模型，以GNN为核心，执行多分类任务（未被标记的肽段上的残基记为0，被标记的肽段上的非标记残基记为1，被标记的肽段上的标记残基记为2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee1058-f92c-4e36-940d-f8417de1acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:/Proteomics/PhD_script/1. Dizco/')\n",
    "sys.path.append('D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/Ligand Discovery/fragment-embedding/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from fragmentembedding import FragmentEmbedder\n",
    "from Protein2Graph import prot2graph\n",
    "from StructureInformationIntegration import data_integration\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,label_binarize\n",
    "from sklearn.metrics import roc_auc_score,precision_score,recall_score,f1_score,accuracy_score,matthews_corrcoef,average_precision_score,auc,precision_recall_curve\n",
    "from models import dizco_GNN\n",
    "from torch.optim import lr_scheduler\n",
    "from EarlyStopping import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from rdkit import RDLogger\n",
    "import pickle\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704f2d3-8f64-40bc-9bda-e0cb57f80a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取蛋白的结构数据为特征一，将每个蛋白都构建成一张图进行GNN训练，同时加入probe的embedding数据，以及每一个氨基酸的结构信息\n",
    "#对每个蛋白的氨基酸位点进行标记\n",
    "#质谱有打到但预测评分小于0.85的肽段(不论是否有修饰)上的氨基酸全部标记为0\n",
    "#对于有修饰的肽段，修饰位点标为2，其余标为1\n",
    "#没有修饰但评分高于0.85的肽段单拎出来作为最后的预测数据\n",
    "def ResLabel(table,thre=0.85):\n",
    "    label_table = table[table['lg_probs']>thre]\n",
    "    label_table = label_table[~label_table['label_site'].isna()]\n",
    "    unlabel_table = table[table['lg_probs']<=thre]\n",
    "    \n",
    "    labeledPEP = ParseRes(label_table,state='labeled')\n",
    "    unlabeledPEP = ParseRes(unlabel_table,state='unlabeled')\n",
    "    res_table = pd.concat([labeledPEP,unlabeledPEP],axis=0)\n",
    "    res_table = res_table.sort_values(by='label',ascending=False).reset_index(drop=True)\n",
    "    res_table = res_table.drop_duplicates(subset=['Accession','res_site']).reset_index(drop=True)\n",
    "    \n",
    "    prot_lt = list(res_table['Accession'].unique())\n",
    "    prot_label = {}\n",
    "    for prot in prot_lt:\n",
    "        seq = uniprot_infor[uniprot_infor['Entry']==prot].iloc[0,-1]\n",
    "        prot_dict = {f'{p}{i+1}':np.nan for i,p in enumerate(seq)}\n",
    "        prot_label.setdefault(prot,prot_dict)\n",
    "    \n",
    "    for prot,table in res_table.groupby(by='Accession'):\n",
    "        for site,label in zip(table['res_site'].to_list(),table['label'].to_list()):\n",
    "            prot_label[prot][site] = label\n",
    "\n",
    "    return prot_label\n",
    "    \n",
    "def ParseRes(data,state='labeled'):\n",
    "    pep_label_result = pd.DataFrame()\n",
    "    for (prot,pep),table in data.groupby(by=['Master Protein Accessions','Upper_Seq']):\n",
    "        try: seq = uniprot_infor[uniprot_infor['Entry']==prot].iloc[0,-1]\n",
    "        except: continue\n",
    "\n",
    "        start = seq.find(pep)+1\n",
    "        label_site = list(table['label_site'].unique())\n",
    "        pep_label = []\n",
    "        for i,p in enumerate(pep):\n",
    "            if state == 'labeled':\n",
    "                if f'{p}{i+1}' in label_site:\n",
    "                    pep_label.append(tuple((prot,f'{p}{start+i}',2)))\n",
    "                else: pep_label.append(tuple((prot,f'{p}{start+i}',1)))\n",
    "            elif state == 'unlabeled' : pep_label.append(tuple((prot,f'{p}{start+i}',0)))\n",
    "        pep_label = pd.DataFrame(pep_label,columns=['Accession','res_site','label'])\n",
    "        pep_label_result = pd.concat([pep_label_result,pep_label],axis=0)\n",
    "    pep_label_result = pep_label_result.sort_values(by='label',ascending=False).reset_index(drop=True)\n",
    "   \n",
    "    return pep_label_result.drop_duplicates(subset=['Accession','res_site'])\n",
    "    \n",
    "def prot2pdb(prot_list,pdb_path):\n",
    "    prot_pdb_list,prot_list_ = [],[]\n",
    "    for name in listdir(pdb_path):\n",
    "        if any(prot in name for prot in prot_list):\n",
    "            if name.split('-')[1] not in prot_list_:\n",
    "                prot_pdb_list.append(name)\n",
    "                prot_list_.append(name.split('-')[1])\n",
    "    return prot_pdb_list\n",
    "\n",
    "def DataSetSplit(dataSet):\n",
    "    data_index = np.array(range(len(dataSet)))\n",
    "    \n",
    "    tv_index, test_index = train_test_split(data_index,test_size=0.1,random_state=42,shuffle=True)\n",
    "    train_index, val_index = train_test_split(tv_index,test_size=1/9,random_state=42,shuffle=True)\n",
    "    extract = lambda x:[dataSet[i] for i in x]\n",
    "    train_set,val_set,test_set = extract(train_index),extract(val_index),extract(test_index)\n",
    "    train_set = DataCombine(train_set)\n",
    "    val_set = DataCombine(val_set)\n",
    "    test_set = DataCombine(test_set,sample_type='test_set')\n",
    "    \n",
    "    return train_set,val_set,test_set\n",
    "    \n",
    "def DataCombine(data,sample_type=None):\n",
    "    graph_set,probe_emd_set,label_set = [],[],[]\n",
    "    str_df = pd.DataFrame()\n",
    "    for i,samples in enumerate(data):\n",
    "        prot_graph,str_table,probeEmbedding,labels = samples\n",
    "        str_table_copy = str_table.copy()\n",
    "        str_table_copy.insert(0,'sample_index',i)\n",
    "        str_df = pd.concat([str_df,str_table_copy],axis=0)\n",
    "        probeEmbedding = np.asarray(probeEmbedding).astype(np.float32)\n",
    "        labels = np.asarray(labels).astype(np.float32)\n",
    "        \n",
    "        graph_set.append(prot_graph)\n",
    "        probe_emd_set.append(torch.tensor(probeEmbedding))\n",
    "        label_set.append(torch.LongTensor(labels))\n",
    "    \n",
    "    str_df = pd.get_dummies(str_df, columns = ['Secondary structure'],dtype=int)\n",
    "    str_set = []\n",
    "    for i in range(len(data)):\n",
    "        str_table = str_df[str_df['sample_index']==i]\n",
    "        str_table = StandardScaler().fit_transform(str_table.iloc[:,3:])\n",
    "        str_table = np.asarray(str_table).astype(np.float32)\n",
    "        str_set.append(torch.tensor(str_table))\n",
    "    \n",
    "    if sample_type is None:\n",
    "        graph_set = [tuple((i,graph)) for i,graph in enumerate(graph_set)]\n",
    "        graph_set = DataLoader(graph_set, batch_size=16, drop_last=False, shuffle=True)\n",
    "\n",
    "    return (graph_set,str_set,probe_emd_set,label_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef2eaa-718a-4853-8605-33ce54939449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为进行多分类任务，所以对应的损失函数也有相应改变，模型最后一层的输出需要用的softmax\n",
    "def model_train(model,train_set,val_set,save_best_model=True,model_path=None,save_path=None,earlyStop=True,epochs=100,lr=0.0001):\n",
    "    if save_path is not None: early_stopping = EarlyStopping(save_path)\n",
    "    else: earlyStop = False\n",
    "    metrics_name = ['train_loss','val_loss','acc','pre0', 'pre1', 'pre2','recall','f1','ap','mcc','auc_score','prc_score_0','prc_score_1','prc_score_2']\n",
    "    #定义损失函数计算方法，定义优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-3)\n",
    "    scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=lr/10, max_lr=lr*10,\n",
    "                                      cycle_momentum=False,step_size_up=len(train_set[0]))\n",
    "    \n",
    "    prot_graphs,prot_struct,probe,labels = train_set\n",
    "    metrics_result = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx,graph_values in prot_graphs:\n",
    "            struct_values = torch.cat([prot_struct[i] for i in batch_idx],axis=0)\n",
    "            probe_values = torch.cat([probe[i] for i in batch_idx],axis=0)\n",
    "            label_values = torch.cat([labels[i] for i in batch_idx],axis=0)\n",
    "            graph_values, struct_values, probe_values, label_values = graph_values.to(device), struct_values.to(device), probe_values.to(device), label_values.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(graph_values, struct_values, probe_values)\n",
    "            loss = criterion(output, label_values)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_loss = total_loss / len(train_set)\n",
    "        eval_matrics = model_eval(model,val_set,criterion)\n",
    "        metrics_result.append(tuple((train_loss,))+eval_matrics)\n",
    "        \n",
    "        if earlyStop:\n",
    "            early_stopping(eval_matrics[0], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                return pd.DataFrame(metrics_result,columns=metrics_name)\n",
    "            break\n",
    "        \n",
    "        if epoch == 0:\n",
    "            best_mcc = eval_matrics[7]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_metrics = tuple((epoch,train_loss))+eval_matrics\n",
    "        else:\n",
    "            if eval_matrics[7] > best_mcc:\n",
    "                best_mcc = eval_matrics[7]\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_metrics = tuple((epoch,train_loss))+eval_matrics\n",
    "        \n",
    "    if save_best_model:\n",
    "        torch.save(best_model.state_dict(),model_path+'best_model.pth')\n",
    "        print('The metrics of best model are:')\n",
    "        print(f'epoch: {best_metrics[0]}, train_loss: {best_metrics[1]}, val_loss: {best_metrics[2]}')\n",
    "        print(f'acc: {best_metrics[3]}, pre0: {best_metrics[4]}, pre1: {best_metrics[5]}, pre2: {best_metrics[6]}')\n",
    "        print(f'recall: {best_metrics[7]}, f1: {best_metrics[8]}, ap: {best_metrics[9]}')\n",
    "        print(f'mcc: {best_metrics[10]}, AUC: {best_metrics[11]}, PRC0: {best_metrics[12]}, PRC1: {best_metrics[13]}, PRC2: {best_metrics[14]}')\n",
    "        \n",
    "    return pd.DataFrame(metrics_result,columns=metrics_name)\n",
    "        \n",
    "def model_eval(model,val_set,criterion,thre=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true,y_pred,y_prob = [],[],[]\n",
    "    prot_graphs,prot_struct,probe,labels = val_set\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,graph_values in prot_graphs:\n",
    "            struct_values = torch.cat([prot_struct[i] for i in batch_idx],axis=0)\n",
    "            probe_values = torch.cat([probe[i] for i in batch_idx],axis=0)\n",
    "            label_values = torch.cat([labels[i] for i in batch_idx],axis=0)\n",
    "            graph_values, struct_values, probe_values, label_values = graph_values.to(device), struct_values.to(device), probe_values.to(device), label_values.to(device)\n",
    "            \n",
    "            output = model(graph_values, struct_values, probe_values)\n",
    "            loss = criterion(output, label_values)\n",
    "            total_loss += loss.item()\n",
    "            prob = F.softmax(output, dim=1).detach().cpu().numpy()\n",
    "            y_true.extend(label_values.detach().tolist())\n",
    "            y_pred.extend(output.argmax(dim=1).detach().cpu().numpy())\n",
    "            y_prob.extend(prob)\n",
    "    \n",
    "        val_loss = total_loss/len(val_set)\n",
    "        eval_matrics = metrics_calculation(y_true,y_pred,y_prob)\n",
    "        \n",
    "        return tuple((val_loss,))+eval_matrics\n",
    "\n",
    "def metrics_calculation(y_true,y_pred,y_prob):\n",
    "    y_true, y_pred, y_prob = np.float64(y_true), np.float64(y_pred), np.float64(y_prob)\n",
    "\n",
    "    auc_score = roc_auc_score(y_true,y_prob,multi_class='ovr')\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    pre0,pre1,pre2 = precision_score(y_true,y_pred,average=None)\n",
    "    recall = recall_score(y_true,y_pred,average='macro')\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "    mcc = matthews_corrcoef(y_true,y_pred)\n",
    "    ap = average_precision_score(y_true,y_prob,average='macro')\n",
    "    y_true_binarized = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    prc_score_lt = []\n",
    "    for i in range(3):\n",
    "        tpr,fpr,_ = precision_recall_curve(y_true_binarized[:,i],y_prob[:,i])\n",
    "        prc_score = auc(fpr,tpr)\n",
    "        prc_score_lt.append(prc_score)\n",
    "    \n",
    "    return tuple((acc,pre0,pre1,pre2,recall,f1,ap,mcc,auc_score,prc_score_lt[0],prc_score_lt[1],prc_score_lt[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f06615-802d-4b58-890c-6a7cd51db3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 把八种探针的PSM数据整合\n",
    "path = 'D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/'\n",
    "file_path = f'{path}Test files/'\n",
    "merge_PSM = pd.read_csv(f'{file_path}merge_isoTOP_PSM_data.csv')\n",
    "uniprot_infor = pd.read_csv(f'{file_path}uniprotkb_human_AND_reviewed_true_AND_m_2024_09_12.tsv',sep='\\t')\n",
    "uniprot_infor['Gene Names'] = uniprot_infor['Gene Names'].str.split(' ',expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0376cf-a926-440f-a2ad-6daab7c48805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 为八种探针鉴定到的所有蛋白创建图\n",
    "pdb_path = f'{path}AlphaFold_pdbFiles/'\n",
    "processed_path = f'{path}/prot_raw_graph_coord/'\n",
    "prot_list = merge_PSM['Master Protein Accessions'].unique()\n",
    "prot_list = [prot for prot in prot_list if not uniprot_infor[uniprot_infor['Entry']==prot].empty]\n",
    "prot_pdb_list = prot2pdb(prot_list,pdb_path)\n",
    "\n",
    "pg = prot2graph(processed_path)\n",
    "with Pool(12) as p:\n",
    "    p.map(pg.CreateCoordGraph,tqdm(prot_pdb_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883af30e-5d0b-4650-9a26-f3ed49a31623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 为八种探针鉴定到的所有蛋白总结结构信息\n",
    "with Pool(8) as p:\n",
    "    structure_data = p.map(data_integration,prot_pdb_list)\n",
    "structure_data_dic = {}\n",
    "for data in structure_data: structure_data_dic.update(data)\n",
    "\n",
    "pickle.dump(structure_data_dic,open(f'{path}pkl_files/structure_infor_summary.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab4f71-88a2-42f3-9e77-35cf2bbcd037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 为每个探针生成一个embedding\n",
    "probe_smile = {\n",
    "    \n",
    "    'AJ5': 'C#CCCC1(N=N1)CCC(NC)=O',\n",
    "    'AJ8': 'C#CCCC1(N=N1)CCC(NC[C@H]2[C@@H](C)CCCN2CC3=CC=C(OC)C=C3)=O',\n",
    "    'AJ12': 'C#CCCC1(N=N1)CCNC(/C(C2=CC=CC=C2)=C/C3=CC=CC=C3)=O',\n",
    "    'AJ14': 'C#CCCC1(N=N1)CCC(NC(C2(C[C@H](C3)C4)C[C@H]4C[C@H]3C2)C)=O',\n",
    "    'AJ22': 'C#CCCC1(N=N1)CCNC(/C(CC)=C/C2=CC=CC([N+]([O-])=O)=C2)=O',\n",
    "    'AJ32': 'C#CCCC1(N=N1)CCNC(C2(CC2)C3=CC(OC(F)(F)O4)=C4C=C3)=O',\n",
    "    'AJ39': 'C#CCCC1(N=N1)CCNC(CCC2=NC(C3=CC=CC=C3)=C(C4=CC=CC=C4)O2)=O',\n",
    "    'CP78': 'C#CCCC1(N=N1)CCC(N2C(CC3=CC=CC=C3)CCCC2)=O'\n",
    "    \n",
    "    }\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "fe = FragmentEmbedder()\n",
    "probe_emd = {probe:fe.transform([smile]) for probe,smile in probe_smile.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07eee4d-0190-461f-97ec-55e53b3988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 根据每个探针找到的肽段以及binding sites的差异，重新定义图，并整合数据\n",
    "#有的蛋白未收录在AlphaFold中，或许是由于uniprot更新了ID\n",
    "str_infor_dic = pickle.load(open(f'{path}pkl_files/structure_infor_summary.pkl','rb'))\n",
    "\n",
    "dataSet = []\n",
    "for probe,table in tqdm(merge_PSM.groupby(by='probe')):\n",
    "    prot_label = ResLabel(table,thre=0.85)\n",
    "    for prot,labels in prot_label.items():\n",
    "        try: prot_graph = torch.load(f'{processed_path}{prot}.pt')\n",
    "        except:\n",
    "            print(f'The graph of {prot} could not be found, please check whether there is a PDB file of it')\n",
    "            continue\n",
    "        node_update = np.array([tuple((int(res[1:])-1,label)) for res,label in labels.items() if not np.isnan(label)])\n",
    "        \n",
    "        try: new_edge,_ = subgraph(subset=torch.tensor(node_update[:,0]),edge_index=prot_graph.edge_index,relabel_nodes=True)\n",
    "        except:\n",
    "            print(f'{prot}')\n",
    "            print(f'No. res in PDB: {prot_graph.x.shape[0]}')\n",
    "            print(f'No. res in Uniprot: {len(labels)}')\n",
    "            continue\n",
    "            \n",
    "        new_X = prot_graph.x[torch.tensor(node_update[:,0])]\n",
    "        new_Y = node_update[:,1]\n",
    "        new_graph = Data(x=new_X,edge_index=new_edge)\n",
    "        \n",
    "        str_table = str_infor_dic[prot].copy()\n",
    "        str_table = str_table.iloc[node_update[:,0],:]\n",
    "        \n",
    "        probeEmbedding = probe_emd[probe]\n",
    "        probeEmbedding = probeEmbedding.repeat(len(node_update),axis=0)\n",
    "        \n",
    "        dataSet.append(tuple((new_graph,str_table,probeEmbedding,new_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a78951-314a-40ec-83e5-6c506e987632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 划分训练集、验证集和测试集\n",
    "train_set,val_set,test_set = DataSetSplit(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc5425-f8c6-4c2e-9790-dd3b6066ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 模型训练与评估\n",
    "graph_dim,struct_dim,probe_dim = test_set[0][0].x.shape[1],test_set[1][0].shape[1],test_set[2][0].shape[1]\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = 'E:/Proteomics/PhD_script/1. Dizco/'\n",
    "\n",
    "dizco_gnn = dizco_GNN(graph_dim,struct_dim,probe_dim).to(device)\n",
    "metrics_result = model_train(dizco_gnn,train_set,val_set,model_path=model_path,lr=0.0001)\n",
    "\n",
    "metrics_result.to_csv('D:/All_for_paper/1. PhD Work Program/3. Research project/1. Dizco/Model_Metrics_Result/Adam_lr(1e-5)_batch(16)(GNN).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc55377-1e07-4baf-83f2-9d567ef0a0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
